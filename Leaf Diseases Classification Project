# 데이터 분할을 위한 폴더 생성
import os
import shutil

original_dataset_dir = "./dataset"
classes_list = os.listdir(original_dataset_dir) # 해당 경로 하위의 모든 폴더 목록 불러온다

base_dir = "./splitted"
os.mkdir(base_dir)

train_dir = os.path.join(base_dir, 'train')
os.mkdir(train_dir)
validation_dir = os.path.join(base_dir, 'val')
os.mkdir(validation_dir)
test_dir = os.path.join(base_dir, 'test')
os.mkdir(test_dir)

for clss in classes_list:
  os.mkdir(os.path.join(train_dir,clss))
  os.mkdir(os.path.join(validation_dir, clss))
  os.mkdir(os.path.join(test_dir, clss))
  
  
 import math
for clss in classes_list:
  path = os.path.join(original_dataset_dir, clss)
  fnames = os.listdir(path)

  train_size = math.floor(len(fnames)*0.6)
  validation_size = math.floor(len(fnames)*0.2)
  test_size = math.floor(len(fnames)*0.2)

  train_fnames = fnames[:train_size]
  for fname in train_fnames:
    src = os.path.join(path.fname)
    dst = os.path.join(os.path.join(train_dir, clss), fname)
    shutil.copyfile(src, dst)

  validation_fnames = fnames[train_size : (validation_size + train_size)]
  for fname in train_fnames:
    src = os.path.join(path.fname)
    dst = os.path.join(os.path.join(train_dir, clss), fname)
    shutil.copyfile(src, dst)

  test_fnames = fnames[(validation_size + train_size) : (validation_size + train_size + test_size)]
  for fname in train_fnames:
    src = os.path.join(path.fname)
    dst = os.path.join(os.path.join(train_dir, clss), fname)
    shutil.copyfile(src, dst)


import torch

USE_CUDA = torch.cuda.is_available()
DEVICE = torch.device("cuda" if USE_CUDA else "cpu")

BATCH_SIZE = 256
EPOCH = 30

import torchvision.transforms as transforms
from torchvision.datasets import ImageFolder

transform_base = transforms.Compose([transforms.Resize((64,64)), transforms.ToTensor()])
train_dataset = ImageFolder(root="./splitted/train",transform=transform_base)
val_dataset = ImageFolder(root="./splitted/val",transform=transform_base)

from torch.utils.data import DataLoader

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers=4)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers=4)


import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

class Net(nn.Module):
  def __init__(self):
    super(Net, self).__init__()
    self.conv1 = nn.Conv2d(3,32,3,padding=1)
    self.pool = nn.MaxPool2d(2,2)
    self.conv2 = nn.Conv2d(32,64,3, padding=1)
    self.conv3 = nn.Conv2d(64,64,3, padding=1)

    self.fc1 = nn.Linear(4096,512)
    self.fc2 = nn.Linear(512,33)

  def forward(self, x):
    x = self.conv1(x)
    x = F.relu(x)
    x = self.pool(x)
    x = F.dropout(x, p=0.25, training = self.training)

    x = self.conv2(x)
    x = F.relu(x)
    x = self.pool(x)
    x = F.dropout(x, p=0.25, training = self.training)

    x = self.conv3(x)
    x = F.relu(x)
    x = self.pool(x)
    x = F.dropout(x, p=0.25, training = self.training)

    x = x.view(-1, 4096)
    x = self.fc1(x)
    x = F.relu(x)
    x = F.dropout(x, p=0.5, traning = self.training)
    x = self.fc2(x)

    return F.log_softmax(x, dim=1)

model_base = Net().to(DEVICE)
optimizer = optim.Adam(model_base.parameters(), lr=0.001)

# 모델 학습을 위한 함수
def train(model, train_loader, optimizer):
  model.train()
  for batch_idx, (data, target) in enumerate(train_loader):
    data, target = data.to(DEVICE), target.to(DEVICE)
    optimizer.zero_grad()
    output = model(data)
    loss = F.cross_entropy(output, target)
    loss.backward()
    optimizer.step()
    
 
 
 

# 모델 평가를 위한 함수
def evaluate(model, test_loader):
  model.eval()
  test_loss = 0
  correct = 0

  with torch.no_grad():
    for data, target in test_loader:
      data, target = data.to(DEVICE), target.to(DEVICE)
      output = model(data)

      test_loss += F.cross_entropy(output, target, reduction='sum').item()
      pred = output.max(1, keepdim=True)[1]
      correct += pred.eq(target.view_as(pred)).sum().item()

  test_loss /= len(test_loader.dataset)
  test_accuracy = 100 * correct / len(test_loader.dataset)
  return test_loss, test_accuracy
  
  
# 모델 학습 실행하기
import time
import copy

def train_baseline(model, train_loader, val_loader, optimizer, num_epochs=30):
  best_acc = 0.0
  best_model_wts = copy.deepcopy(model.state_dict())

  for epoch in range(1, num_epochs+1):
    since = time.time()
    train(model, train_loader, optimizer)
    train_loss, train_acc = evaluate(model, train_loader)
    val_loss, val_acc = evaluate(model, val_loader)

    if val_acc > best_acc:
      best_acc = val_acc
      best_model_wts = copy.deepcopy(model.state_dict())

    time_elapsed = time.time() - since

    print("---------epoch{}-----------".format(epoch))
    print("train_loss : {:.4f}, Accuracy : {:.2f}%".format(train_loss, train_acc))
    print("val_loss : {:.4f}, Accuracy : {:.2f}%".format(val_loss, val_acc))
  model.load_state_dict(best_model_wts)
  return model

base = train_baseline(model_base, train_loader, val_loader, optimizer, EPOCH)
torch.save(base, "baseline.pt")

# Transfer Learning을 위한 준비

data_transforms = {
    'train' : transforms.Compose([
                transforms.Resize([64,64]),
                transforms.RandomHorizontalFlip(),
                transforms.RandomVerticalFlip(),
                transforms.RandomCrop(52),
                transforms.ToTensor(),
                transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
    ]),
    'val' : transforms.Compose([
                transforms.Resize([64,64]),
                transforms.RandomCrop(52),
                transforms.ToTensor(),
                transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
    ])
}

data_dir = "./splitted"
image_datasets = {x:ImageFolder(root=os.path.join(data_dir, x), transform=data_transforms[x]) for x in ['train','val']}
dataloaders = { x : torch.utils.data.DataLoader(image_datasets[x], batch_size = BATCH_SIZE, shuffle = True, num_workers=4) for x in ['train'.'val']}
dataset_sizes = {x : len(image_datasets[x]) for x in ['train','val']}
class_names = image_datasets['train'].classes


from torchvision import models

resnet = models.resnet50(pretrained = True)
num_ftrs = resnet.fc.in_features
resnet.fc = nn.Linear(num_ftrs, 33)
resnet = resnet.to(DEVICE)

criterion = nn.CrossEntropyLoss()

optimizer_ft = optim.Adam(filter(lambda p:p.requires_grad, resnet.parameters()), lr=0.001)

from torch.optim import lr_scheduler

exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)

# Pre-Trained Model의 일부 Layer를 Freeze하기

ct = 0
for child in resnet.children():
  ct += 1
  if ct < 6:
    for param in child.parameters():
      param.requires_grad = False
      
  
  

# Transfer Learning 모델 학습과 검증을 위한 함수

def train_resnet(model, criterion, optimizer, scheduler, num_epochs=25):
  best_model_wts = copy.deepcopy(model.state_dict())
  best_acc = 0.0

  for epoch in range(num_epochs):
    print("--------epoch {}----------".format(epoch+1))
    since = time.time()

    for phase in ['train','val']:
      if phase == 'train':
        model.train()
      else:
        model.eval()

      running_loss = 0.0
      running_corrects = 0

      for inputs, labels in dataloaders[phase]:
        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)

        optimizer.zero_grad()

        with torch.set_grad_enabled(phase == 'train'):
          outputs = model(inputs)
          preds = torch.max(outputs, 1)[1]
          loss = criterion(outputs, labels)

          if phase == 'train':
            loss.backward()
            optimizer.step()

        running_loss += loss.item() * input_size(0)
        running_corrects += torch.sum(preds == labels.data)

      if phase == 'train':
        scheduler.step()
        l_r = [x['lr'] for x in optimizer_ft.param_groups]
        print("learning rate : ",l_r)

      epoch_loss = running_loss / dataset_sizes[phase]
      epoch_acc = running_corrects.double() / dataset_sizes[phase]

      print("{} Loss : {:.4f} Acc : {:.4f}".format(phase, epoch_loss, epoch_acc))

      if phase == 'val' and epoch_acc > best_acc:
        best_acc = epoch_acc
        best_model_wts = copy.deepcopy(model.state_dict())

    time_elapsed = time.time() - since
    print("Completed in {:.0f}m {:.0f}s".format(time_elapsed // 60, time_elapsed%60))
  print("Best val Acc : {:.4f}".format(best_acc))

  model.load_state_dict(best_model_wts)

  return model
  

#모델 학습 실행하기

model_resnet50 = train_resnet(resnet, criterion.optimizer_ft, exp_lr_scheduler, num_epochs=EPOCH)
torch.save(model_resnet50, 'resnet50.pt')

# 만든 모델을 Test dataset을 가지고 평가하기
transform_base = transforms.Compose([ transforms.Resize([64,64]), transforms.ToTensor()])
test_base = ImageFolder(root="./splitted/test", transforms=transform_base)
test_loader_base = torch.utils.data.DataLoader(test_base, batch_size = BATCH_SIZE, shuffle = True, num_workers=4)


from torchvision.transforms.transforms import RandomCrop
# Transfor Learning모델 평가를 위한 전처리하기
transform_resNet = transforms.Compose([
          transforms.Resize([64,64]),                            
          transforms.RandomCrop(52),
          transforms.ToTensor(),
          transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]) 
])

test_resNet = ImageFolder(root = "./splitted/test", transform=transform_resNet)
test_loader_resNet = torch.utils.data.DataLoader(test_resNet, batch_size = BATCH_SIZE, shuffle = True, num_workers=4)


# 베이스라인 모델 성능 평가

baseline = torch.load('baseline.pt')
baseline.eval()
test_loss, test_accuracy = evaluate(baseline, test_loader)

print("baseline test acc : ", test_accuracy)

# Transfer Learning 모델 평가

resnet50 = torch.load("resnet50.pt")
resnet50.eval()
test_loss, test_accuracy = evaluate(resnet50, test_loader_resNet)

print("ResNet test acc : ", test_accuracy)



